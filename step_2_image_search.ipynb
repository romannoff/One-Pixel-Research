{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ea4dc148-d913-47db-bc32-74aa6784954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "import lightning as L\n",
    "from lightning import Fabric\n",
    "from src.resnet_modifications import resnet18\n",
    "from src.models import AlexNetInception1x1, AlexNetSeparable11, AlexNetSkipConnection, AlexNetWithBatchNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e36bdb85-29b2-4780-9eb4-e77d0bb18443",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = 'data/img_align_celeba/img_align_celeba/'\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "760c47a2-d19b-4e2e-8d91-a0c64c759e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = pd.read_csv('data/list_attr_celeba.csv')\n",
    "split = pd.read_csv('data/list_eval_partition.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbfd16ba-9349-470a-bc71-b4cf49351898",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = split[split['partition'] == 2].image_id.values\n",
    "y_test = ys[ys['image_id'].isin(X_test)]['Heavy_Makeup'].values\n",
    "y_test = np.where(y_test == -1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "334503cd-abb2-40c2-a431-920a20b6fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6af020d-de94-4cd7-a8f5-4815220d0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(current_model):\n",
    "    current_model.eval()\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in tqdm(test_loader):\n",
    "            imgs, targets = imgs.to(DEVICE), targets.float().to(DEVICE)\n",
    "            outputs = current_model(imgs).squeeze(1)\n",
    "\n",
    "            preds = torch.sigmoid(outputs).cpu().numpy() > 0.5\n",
    "            all_preds.extend(preds.astype(int))\n",
    "            all_targets.extend(targets.cpu().numpy().astype(int))\n",
    "\n",
    "    return np.array(all_targets), np.array(all_preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94ac0843-bd24-4ad9-9945-e601e824ee7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeupDataset(Dataset):\n",
    "    def __init__(self, image_ids, labels, root_dir, transform=None):\n",
    "        self.ids = image_ids\n",
    "        self.labels = labels\n",
    "        self.root = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_id = self.ids[idx]\n",
    "        path = os.path.join(self.root, img_id)\n",
    "        img = Image.open(path).convert('RGB')\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        label = self.labels[idx]\n",
    "        return img, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e711056-e273-4a8e-801d-2ea87918e581",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds  = MakeupDataset(X_test,  y_test,  DATA_DIR, transform=val_tfms)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=64, shuffle=False, num_workers=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f689265-5038-4eac-960b-bfabd2fda28d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "baecea74-3528-4b97-b8cc-5fba8076b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla\n",
    "alexnet_model = models.alexnet()\n",
    "alexnet_model.classifier[6] = nn.Linear(4096, 1) \n",
    "alexnet_model.load_state_dict(torch.load('models/best_alexnet.pth'))\n",
    "alexnet_model.eval()\n",
    "alexnet_model.to(DEVICE)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dbf34a63-4e5f-46cd-aea0-9051032edc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain\n",
    "alexnet_model_pretrain = models.alexnet()\n",
    "alexnet_model_pretrain.classifier[6] = nn.Linear(4096, 1) \n",
    "alexnet_model_pretrain.load_state_dict(torch.load('models/best_alexnet_pretrain.pth'))\n",
    "alexnet_model_pretrain.eval()\n",
    "alexnet_model_pretrain.to(DEVICE)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d25657a-8a82-4342-b689-3ac05f4cdccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 1x1 kernels\n",
    "alexnet_model_1x1 = AlexNetInception1x1()\n",
    "alexnet_model_1x1.load_state_dict(torch.load('models/best_alexnet_1x1.pth'))\n",
    "alexnet_model_1x1.eval()\n",
    "alexnet_model_1x1.to(DEVICE)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4ab056b-e950-45b4-a5fd-239916581d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with 11x1 and 1x11\n",
    "alexnet_model_1x11 = AlexNetSeparable11()\n",
    "alexnet_model_1x11.load_state_dict(torch.load('models/best_alexnet_11.pth'))\n",
    "alexnet_model_1x11.eval()\n",
    "alexnet_model_1x11.to(DEVICE)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "186c9496-2767-42f9-ac56-0d63523e2a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with scip-connection\n",
    "alexnet_model_sc = AlexNetSkipConnection()\n",
    "alexnet_model_sc.load_state_dict(torch.load('models/best_alexnet_skip_connection.pth'))\n",
    "alexnet_model_sc.eval()\n",
    "alexnet_model_sc.to(DEVICE)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8635b67b-4a74-4cfb-ae41-e588f458919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with BatchNorm\n",
    "alexnet_model_bn = AlexNetWithBatchNorm()\n",
    "alexnet_model_bn.load_state_dict(torch.load('models/best_alexnet_bn.pth'))\n",
    "alexnet_model_bn.eval()\n",
    "alexnet_model_bn.to(DEVICE)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "13bc0d71-47e9-4e11-a289-865b940a3589",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with pixels\n",
    "alexnet_model_pixels = models.alexnet()\n",
    "alexnet_model_pixels.classifier[6] = nn.Linear(4096, 1) \n",
    "alexnet_model_pixels.load_state_dict(torch.load('models/best_alexnet_pixels.pth'))\n",
    "alexnet_model_pixels.eval()\n",
    "alexnet_model_pixels.to(DEVICE)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f28749bc-0f61-413a-9882-749ff3413ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 312/312 [00:06<00:00, 45.34it/s]\n",
      "100%|█████████████████████████████████████████| 312/312 [00:06<00:00, 46.17it/s]\n",
      "100%|█████████████████████████████████████████| 312/312 [00:07<00:00, 42.21it/s]\n",
      "100%|█████████████████████████████████████████| 312/312 [00:08<00:00, 38.41it/s]\n",
      "100%|█████████████████████████████████████████| 312/312 [00:05<00:00, 53.26it/s]\n",
      "100%|█████████████████████████████████████████| 312/312 [00:07<00:00, 44.35it/s]\n",
      "100%|█████████████████████████████████████████| 312/312 [00:06<00:00, 45.98it/s]\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred_alexnet = get_preds(alexnet_model)\n",
    "_, y_pred_alexnet_pretrain = get_preds(alexnet_model_pretrain)\n",
    "_, y_pred_alexnet_1x1 = get_preds(alexnet_model_1x1)\n",
    "_, y_pred_alexnet_1x11 = get_preds(alexnet_model_1x11)\n",
    "_, y_pred_alexnet_sc = get_preds(alexnet_model_sc)\n",
    "_, y_pred_alexnet_bn = get_preds(alexnet_model_bn)\n",
    "_, y_pred_alexnet_pixels = get_preds(alexnet_model_pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a40ec70b-358e-4e74-b51d-82200185907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_images_mask = (\n",
    "    (y_pred_alexnet == y_true) &\n",
    "    (y_pred_alexnet_pretrain == y_true) &\n",
    "    (y_pred_alexnet_1x1 == y_true) &\n",
    "    (y_pred_alexnet_1x11 == y_true) &\n",
    "    (y_pred_alexnet_sc == y_true) &\n",
    "    (y_pred_alexnet_bn == y_true) & \n",
    "    (y_pred_alexnet_pixels == y_true)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6e756465-7ce4-48c8-bd30-aa97ada99f8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['182638.jpg', '182639.jpg', '182641.jpg', '182642.jpg',\n",
       "       '182643.jpg', '182644.jpg', '182646.jpg', '182647.jpg',\n",
       "       '182648.jpg', '182649.jpg', '182652.jpg', '182653.jpg',\n",
       "       '182655.jpg', '182656.jpg', '182658.jpg', '182659.jpg',\n",
       "       '182660.jpg', '182661.jpg', '182662.jpg', '182663.jpg',\n",
       "       '182664.jpg', '182665.jpg', '182666.jpg', '182667.jpg',\n",
       "       '182669.jpg', '182670.jpg', '182671.jpg', '182672.jpg',\n",
       "       '182673.jpg', '182674.jpg'], dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[true_images_mask][:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c016b89-d6c8-4bd0-9e52-d57f7bd1facd",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_images = (X_test[true_images_mask], y_test[true_images_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1661eef6-feb5-4586-b542-2ce0f58faa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('results/true_images.pickle', 'wb') as f:\n",
    "    pickle.dump(true_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f3810f-2837-41be-be15-f26f3648155b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cf660f4-2e75-4d21-925f-e90ecf039e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "alexnet_model_bn_do = models.alexnet(dropout=0.8)\n",
    "alexnet_model_bn_do.classifier[6] = nn.Linear(4096, 1) \n",
    "alexnet_model_bn_do.load_state_dict(torch.load('models/best_alexnet_do.pth'))\n",
    "alexnet_model_bn_do.eval()\n",
    "alexnet_model_bn_do.to(DEVICE)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a10b902-a039-44ce-ae64-7bed25cb2543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 312/312 [00:06<00:00, 45.56it/s]\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred_alexnet_bn_do = get_preds(alexnet_model_bn_do)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c291d3ee-4b48-413f-9560-f0a530d8486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_images_mask = (\n",
    "    (y_pred_alexnet == y_true) &\n",
    "    (y_pred_alexnet_pretrain == y_true) &\n",
    "    (y_pred_alexnet_1x1 == y_true) &\n",
    "    (y_pred_alexnet_1x11 == y_true) &\n",
    "    (y_pred_alexnet_sc == y_true) &\n",
    "    (y_pred_alexnet_bn == y_true) & \n",
    "    (y_pred_alexnet_pixels == y_true) & \n",
    "    (y_pred_alexnet_bn_do == y_true)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "908e208d-016f-41e2-b43a-64e6eac61fc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['182638.jpg', '182639.jpg', '182641.jpg', '182642.jpg',\n",
       "       '182643.jpg', '182644.jpg', '182646.jpg', '182647.jpg',\n",
       "       '182648.jpg', '182649.jpg', '182652.jpg', '182653.jpg',\n",
       "       '182655.jpg', '182656.jpg', '182658.jpg', '182659.jpg',\n",
       "       '182660.jpg', '182661.jpg', '182662.jpg', '182663.jpg',\n",
       "       '182664.jpg', '182665.jpg', '182666.jpg', '182667.jpg',\n",
       "       '182669.jpg', '182670.jpg', '182671.jpg', '182672.jpg',\n",
       "       '182673.jpg', '182674.jpg'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[true_images_mask][:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c08f0-e1cf-4958-8ef0-9fc39111cff8",
   "metadata": {},
   "source": [
    "# ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8917c11c-d54a-4ec9-b4a8-53b6bf8bc452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vanilla\n",
    "\n",
    "resnet_18_model = models.resnet18()\n",
    "resnet_18_model.fc = nn.Linear(512, 1)\n",
    "resnet_18_model.load_state_dict(torch.load('models/best_resnet18_2.pth'))\n",
    "resnet_18_model.eval()\n",
    "resnet_18_model.to(DEVICE)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3620d1e2-d3a9-4a6b-8ca3-81b70c79678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrain\n",
    "\n",
    "resnet_18_pretrain_model = models.resnet18()\n",
    "resnet_18_pretrain_model.fc = nn.Linear(512, 1)\n",
    "resnet_18_pretrain_model.load_state_dict(torch.load('models/best_resnet18_pretrain.pth'))\n",
    "resnet_18_pretrain_model.eval()\n",
    "resnet_18_pretrain_model.to(DEVICE)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e1fa9fb-1154-4a16-a56c-758142644088",
   "metadata": {},
   "outputs": [],
   "source": [
    "# silu\n",
    "\n",
    "resnet_18_silu_model = resnet18()\n",
    "resnet_18_silu_model.fc = nn.Linear(512, 1)\n",
    "resnet_18_silu_model.load_state_dict(torch.load('models/best_resnet_silu.pth'))\n",
    "resnet_18_silu_model.eval()\n",
    "resnet_18_silu_model.to(DEVICE)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bd0f8295-3a1d-440c-bba7-23269a83863e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet101\n",
    "\n",
    "resnet_101_model = models.resnet101()\n",
    "resnet_101_model.fc = nn.Linear(2048, 1)\n",
    "resnet_101_model.load_state_dict(torch.load('models/best_resnet_101.pth'))\n",
    "resnet_101_model.eval()\n",
    "resnet_101_model.to(DEVICE)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a27e6816-7aa7-4fd1-8166-139a63504a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████| 312/312 [00:13<00:00, 23.82it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 312/312 [00:12<00:00, 24.20it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 312/312 [00:12<00:00, 24.21it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 312/312 [00:56<00:00,  5.56it/s]\n"
     ]
    }
   ],
   "source": [
    "y_true, y_pred_resnet_18 = get_preds(resnet_18_model)\n",
    "_, y_pred_resnet_18_pretrain = get_preds(resnet_18_pretrain_model)\n",
    "_, y_pred_resnet_silu = get_preds(resnet_18_silu_model)\n",
    "_, y_pred_resnet_101 = get_preds(resnet_101_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9f60a50-d3a2-45bb-8025-96be9fb06a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_images_mask = (\n",
    "    (y_pred_resnet_18 == y_true) &\n",
    "    (y_pred_resnet_18_pretrain == y_true) &\n",
    "    (y_pred_resnet_silu == y_true) &\n",
    "    (y_pred_resnet_101 == y_true)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0054203-f423-491f-9e9e-1f023bee72e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_images = (X_test[true_images_mask], y_test[true_images_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01cd2d76-0bd1-4da5-bdb2-19fffa9feba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('results/true_images_resnet.pickle', 'wb') as f:\n",
    "    pickle.dump(true_images, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ddd029-a28b-4a32-be53-f8191dea348b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
